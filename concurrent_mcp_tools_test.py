#!/usr/bin/env python3
"""
âš¡ Concurrent MCP Tools Test - à¸—à¸”à¸ªà¸­à¸šà¹ƒà¸Šà¹‰ MCP Tools à¸«à¸¥à¸²à¸¢à¸•à¸±à¸§à¸à¸£à¹‰à¸­à¸¡à¸à¸±à¸™
================================================================

à¸—à¸”à¸ªà¸­à¸šà¸à¸²à¸£à¹ƒà¸Šà¹‰ Fast_Coding à¹à¸¥à¸° Sequential_Thinking à¸«à¸¥à¸²à¸¢à¸•à¸±à¸§à¸à¸£à¹‰à¸­à¸¡à¸à¸±à¸™à¹ƒà¸™à¹„à¸Ÿà¸¥à¹Œà¹€à¸”à¸µà¸¢à¸§
à¸à¸£à¹‰à¸­à¸¡à¸ˆà¸±à¸šà¹€à¸§à¸¥à¸²à¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸
"""

import asyncio
import aiohttp
import time
import json
from datetime import datetime
from typing import Dict, List, Any
import concurrent.futures
from dataclasses import dataclass

@dataclass
class MCPRequest:
    """à¸„à¸¥à¸²à¸ªà¸ªà¸³à¸«à¸£à¸±à¸šà¹€à¸à¹‡à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥ MCP request"""
    ai_name: str
    tool_name: str
    task: str
    request_id: str
    start_time: float = 0
    end_time: float = 0
    success: bool = False
    response: str = ""

class ConcurrentMCPTester:
    """à¸£à¸°à¸šà¸šà¸—à¸”à¸ªà¸­à¸š MCP Tools à¹à¸šà¸š concurrent"""
    
    def __init__(self):
        self.mcp_tools = {
            "Fast_Coding": "http://localhost:8574",
            "Sequential_Thinking": "http://localhost:8575",
            "Neuroflow_Logs": "http://localhost:8573"
        }
        
        # AI teams à¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸²à¸£à¸—à¸”à¸ªà¸­à¸š
        self.ai_teams = {
            "Fast_Coding_Team": [
                "GitHub Copilot", "Coder AI", "v0 AI", "Windsurf AI", 
                "Bolt AI", "Replit AI", "Fast Coding AI", "MIX IDE AI"
            ],
            "Sequential_Thinking_Team": [
                "Trae AI", "OpenAI GPT", "Anthropic Claude", "NEXUS AI",
                "Sequential Thinking AI", "Complete MCP AI", "Documentation AI", "Security Analyst AI"
            ],
            "Neuroflow_Logs_Team": [
                "Google AI", "Local AI", "Performance Optimizer AI", "Specialized Debugger AI"
            ]
        }
        
        self.test_results = []
        self.performance_metrics = {}
    
    async def run_concurrent_tests(self):
        """à¸£à¸±à¸™à¸à¸²à¸£à¸—à¸”à¸ªà¸­à¸šà¹à¸šà¸š concurrent"""
        
        print("âš¡ Concurrent MCP Tools Test - Starting...")
        print("=" * 60)
        print("Testing multiple MCP Tools simultaneously with timing")
        print()
        
        # Test scenarios
        test_scenarios = [
            {
                "name": "Small Concurrent Load",
                "description": "3 tools, 2 requests each",
                "fast_coding_requests": 2,
                "sequential_thinking_requests": 2,
                "neuroflow_logs_requests": 2
            },
            {
                "name": "Medium Concurrent Load", 
                "description": "3 tools, 4 requests each",
                "fast_coding_requests": 4,
                "sequential_thinking_requests": 4,
                "neuroflow_logs_requests": 4
            },
            {
                "name": "Heavy Concurrent Load",
                "description": "3 tools, 8 requests each",
                "fast_coding_requests": 8,
                "sequential_thinking_requests": 8,
                "neuroflow_logs_requests": 8
            },
            {
                "name": "Extreme Concurrent Load",
                "description": "3 tools, 12 requests each",
                "fast_coding_requests": 12,
                "sequential_thinking_requests": 12,
                "neuroflow_logs_requests": 12
            }
        ]
        
        # à¸£à¸±à¸™à¹à¸•à¹ˆà¸¥à¸° scenario
        for i, scenario in enumerate(test_scenarios, 1):
            print(f"ğŸ¯ Scenario {i}: {scenario['name']}")
            print(f"Description: {scenario['description']}")
            print("-" * 50)
            
            # à¸£à¸±à¸™à¸à¸²à¸£à¸—à¸”à¸ªà¸­à¸š concurrent
            scenario_results = await self.run_scenario(scenario)
            
            # à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ
            self.analyze_scenario_results(scenario, scenario_results)
            
            print("-" * 50)
            print()
            
            # à¸£à¸­à¸ªà¸±à¸à¸„à¸£à¸¹à¹ˆà¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡ scenario
            await asyncio.sleep(2)
        
        # à¸ªà¸£à¸¸à¸›à¸œà¸¥à¸à¸²à¸£à¸—à¸”à¸ªà¸­à¸šà¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”
        self.summarize_all_tests()
    
    async def run_scenario(self, scenario: Dict[str, Any]) -> List[MCPRequest]:
        """à¸£à¸±à¸™à¸à¸²à¸£à¸—à¸”à¸ªà¸­à¸š scenario à¹€à¸”à¸µà¸¢à¸§"""
        
        print(f"âš¡ Starting concurrent requests...")
        
        # à¸ªà¸£à¹‰à¸²à¸‡ requests à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”
        all_requests = []
        
        # Fast_Coding requests
        for i in range(scenario["fast_coding_requests"]):
            ai_name = self.ai_teams["Fast_Coding_Team"][i % len(self.ai_teams["Fast_Coding_Team"])]
            request = MCPRequest(
                ai_name=ai_name,
                tool_name="Fast_Coding",
                task=f"Generate code for feature {i+1} - {scenario['name']}",
                request_id=f"fc_{i+1}_{int(time.time())}"
            )
            all_requests.append(request)
        
        # Sequential_Thinking requests
        for i in range(scenario["sequential_thinking_requests"]):
            ai_name = self.ai_teams["Sequential_Thinking_Team"][i % len(self.ai_teams["Sequential_Thinking_Team"])]
            request = MCPRequest(
                ai_name=ai_name,
                tool_name="Sequential_Thinking",
                task=f"Analyze problem {i+1} step by step - {scenario['name']}",
                request_id=f"st_{i+1}_{int(time.time())}"
            )
            all_requests.append(request)
        
        # Neuroflow_Logs requests
        for i in range(scenario["neuroflow_logs_requests"]):
            ai_name = self.ai_teams["Neuroflow_Logs_Team"][i % len(self.ai_teams["Neuroflow_Logs_Team"])]
            request = MCPRequest(
                ai_name=ai_name,
                tool_name="Neuroflow_Logs",
                task=f"Monitor system {i+1} - {scenario['name']}",
                request_id=f"nl_{i+1}_{int(time.time())}"
            )
            all_requests.append(request)
        
        print(f"ğŸ“Š Total requests: {len(all_requests)}")
        print(f"   â€¢ Fast_Coding: {scenario['fast_coding_requests']}")
        print(f"   â€¢ Sequential_Thinking: {scenario['sequential_thinking_requests']}")
        print(f"   â€¢ Neuroflow_Logs: {scenario['neuroflow_logs_requests']}")
        
        # à¹€à¸£à¸´à¹ˆà¸¡à¸ˆà¸±à¸šà¹€à¸§à¸¥à¸²
        scenario_start_time = time.time()
        
        # à¸£à¸±à¸™ requests à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¸à¸£à¹‰à¸­à¸¡à¸à¸±à¸™
        tasks = [self.execute_mcp_request(request) for request in all_requests]
        completed_requests = await asyncio.gather(*tasks, return_exceptions=True)
        
        # à¸ˆà¸šà¸à¸²à¸£à¸ˆà¸±à¸šà¹€à¸§à¸¥à¸²
        scenario_end_time = time.time()
        scenario_duration = scenario_end_time - scenario_start_time
        
        print(f"â±ï¸ Scenario completed in: {scenario_duration:.2f} seconds")
        
        # à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ
        successful_requests = []
        for result in completed_requests:
            if isinstance(result, MCPRequest):
                successful_requests.append(result)
                self.test_results.append(result)
        
        # à¹€à¸à¹‡à¸š metrics
        self.performance_metrics[scenario["name"]] = {
            "total_requests": len(all_requests),
            "successful_requests": len(successful_requests),
            "failed_requests": len(all_requests) - len(successful_requests),
            "duration": scenario_duration,
            "requests_per_second": len(successful_requests) / scenario_duration if scenario_duration > 0 else 0,
            "success_rate": (len(successful_requests) / len(all_requests)) * 100 if all_requests else 0
        }
        
        return successful_requests
    
    async def execute_mcp_request(self, request: MCPRequest) -> MCPRequest:
        """à¸”à¸³à¹€à¸™à¸´à¸™à¸à¸²à¸£ MCP request à¹€à¸”à¸µà¸¢à¸§"""
        
        request.start_time = time.time()
        
        try:
            tool_url = self.mcp_tools[request.tool_name]
            
            async with aiohttp.ClientSession() as session:
                payload = {
                    "request": request.task,
                    "ai_user": request.ai_name,
                    "request_id": request.request_id,
                    "type": "concurrent_test"
                }
                
                async with session.post(
                    f"{tool_url}/api/process",
                    json=payload,
                    timeout=15  # à¹€à¸à¸´à¹ˆà¸¡ timeout à¸ªà¸³à¸«à¸£à¸±à¸š concurrent requests
                ) as response:
                    request.end_time = time.time()
                    
                    if response.status == 200:
                        data = await response.json()
                        request.success = True
                        request.response = data.get('response', 'Success')
                    else:
                        request.success = False
                        request.response = f"HTTP {response.status}"
        
        except asyncio.TimeoutError:
            request.end_time = time.time()
            request.success = False
            request.response = "Timeout"
        except Exception as e:
            request.end_time = time.time()
            request.success = False
            request.response = str(e)
        
        return request
    
    def analyze_scenario_results(self, scenario: Dict[str, Any], results: List[MCPRequest]):
        """à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¸‚à¸­à¸‡ scenario"""
        
        metrics = self.performance_metrics[scenario["name"]]
        
        print(f"ğŸ“ˆ Scenario Results:")
        print(f"   â€¢ Total Requests: {metrics['total_requests']}")
        print(f"   â€¢ Successful: {metrics['successful_requests']}")
        print(f"   â€¢ Failed: {metrics['failed_requests']}")
        print(f"   â€¢ Success Rate: {metrics['success_rate']:.1f}%")
        print(f"   â€¢ Duration: {metrics['duration']:.2f} seconds")
        print(f"   â€¢ Requests/Second: {metrics['requests_per_second']:.2f}")
        
        # à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸•à¸²à¸¡ tool
        tool_stats = {}
        for result in results:
            tool = result.tool_name
            if tool not in tool_stats:
                tool_stats[tool] = {"total": 0, "success": 0, "avg_time": 0, "times": []}
            
            tool_stats[tool]["total"] += 1
            if result.success:
                tool_stats[tool]["success"] += 1
            
            duration = result.end_time - result.start_time
            tool_stats[tool]["times"].append(duration)
        
        # à¸„à¸³à¸™à¸§à¸“à¹€à¸§à¸¥à¸²à¹€à¸‰à¸¥à¸µà¹ˆà¸¢
        for tool, stats in tool_stats.items():
            if stats["times"]:
                stats["avg_time"] = sum(stats["times"]) / len(stats["times"])
                stats["min_time"] = min(stats["times"])
                stats["max_time"] = max(stats["times"])
        
        print(f"\nğŸ”§ Tool Performance:")
        for tool, stats in tool_stats.items():
            success_rate = (stats["success"] / stats["total"]) * 100 if stats["total"] > 0 else 0
            print(f"   â€¢ {tool}:")
            print(f"     Success: {stats['success']}/{stats['total']} ({success_rate:.1f}%)")
            print(f"     Avg Time: {stats['avg_time']:.3f}s")
            print(f"     Min Time: {stats['min_time']:.3f}s")
            print(f"     Max Time: {stats['max_time']:.3f}s")
    
    def summarize_all_tests(self):
        """à¸ªà¸£à¸¸à¸›à¸œà¸¥à¸à¸²à¸£à¸—à¸”à¸ªà¸­à¸šà¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”"""
        
        print("=" * 60)
        print("ğŸ“Š CONCURRENT MCP TOOLS TEST SUMMARY")
        print("=" * 60)
        
        total_requests = sum(len([r for r in self.test_results if r.success]) for _ in [1])
        total_successful = len([r for r in self.test_results if r.success])
        total_failed = len(self.test_results) - total_successful
        overall_success_rate = (total_successful / len(self.test_results)) * 100 if self.test_results else 0
        
        print(f"ğŸ“ˆ Overall Statistics:")
        print(f"   â€¢ Total Requests: {len(self.test_results)}")
        print(f"   â€¢ Successful: {total_successful}")
        print(f"   â€¢ Failed: {total_failed}")
        print(f"   â€¢ Overall Success Rate: {overall_success_rate:.1f}%")
        
        print(f"\nğŸ¯ Scenario Performance Comparison:")
        for scenario_name, metrics in self.performance_metrics.items():
            status = "ğŸŸ¢" if metrics["success_rate"] == 100 else "ğŸŸ¡" if metrics["success_rate"] >= 80 else "ğŸ”´"
            print(f"   {status} {scenario_name}:")
            print(f"      Requests: {metrics['successful_requests']}/{metrics['total_requests']}")
            print(f"      Success Rate: {metrics['success_rate']:.1f}%")
            print(f"      Duration: {metrics['duration']:.2f}s")
            print(f"      RPS: {metrics['requests_per_second']:.2f}")
        
        print(f"\nâš¡ Performance Analysis:")
        
        # à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸à¸£à¸§à¸¡
        all_durations = [r.end_time - r.start_time for r in self.test_results if r.success]
        if all_durations:
            avg_request_time = sum(all_durations) / len(all_durations)
            min_request_time = min(all_durations)
            max_request_time = max(all_durations)
            
            print(f"   â€¢ Average Request Time: {avg_request_time:.3f}s")
            print(f"   â€¢ Fastest Request: {min_request_time:.3f}s")
            print(f"   â€¢ Slowest Request: {max_request_time:.3f}s")
        
        # à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸•à¸²à¸¡ tool
        tool_performance = {}
        for result in self.test_results:
            tool = result.tool_name
            if tool not in tool_performance:
                tool_performance[tool] = {"requests": 0, "successes": 0, "times": []}
            
            tool_performance[tool]["requests"] += 1
            if result.success:
                tool_performance[tool]["successes"] += 1
                tool_performance[tool]["times"].append(result.end_time - result.start_time)
        
        print(f"\nğŸ”§ Tool Performance Summary:")
        for tool, perf in tool_performance.items():
            success_rate = (perf["successes"] / perf["requests"]) * 100 if perf["requests"] > 0 else 0
            avg_time = sum(perf["times"]) / len(perf["times"]) if perf["times"] else 0
            
            print(f"   â€¢ {tool}:")
            print(f"     Total Requests: {perf['requests']}")
            print(f"     Success Rate: {success_rate:.1f}%")
            print(f"     Avg Response Time: {avg_time:.3f}s")
        
        print(f"\nğŸ¯ Concurrency Insights:")
        
        # à¸«à¸² scenario à¸—à¸µà¹ˆà¸”à¸µà¸—à¸µà¹ˆà¸ªà¸¸à¸”
        best_scenario = max(self.performance_metrics.items(), 
                           key=lambda x: x[1]["requests_per_second"])
        worst_scenario = min(self.performance_metrics.items(), 
                            key=lambda x: x[1]["requests_per_second"])
        
        print(f"   ğŸ† Best Performance: {best_scenario[0]}")
        print(f"      {best_scenario[1]['requests_per_second']:.2f} requests/second")
        
        print(f"   ğŸ“‰ Lowest Performance: {worst_scenario[0]}")
        print(f"      {worst_scenario[1]['requests_per_second']:.2f} requests/second")
        
        if overall_success_rate == 100:
            print(f"\nğŸ‰ Perfect concurrent performance!")
            print(f"All MCP Tools handled concurrent requests successfully!")
        elif overall_success_rate >= 90:
            print(f"\nâœ… Excellent concurrent performance!")
            print(f"MCP Tools handled most concurrent requests well!")
        elif overall_success_rate >= 70:
            print(f"\nğŸ‘ Good concurrent performance!")
            print(f"MCP Tools showed decent concurrent handling!")
        else:
            print(f"\nâš ï¸ Concurrent performance needs improvement!")
            print(f"Consider optimizing MCP Tools for better concurrency!")
        
        print(f"\nğŸ† CONCURRENT TEST COMPLETE!")
        print(f"Tested {len(self.test_results)} concurrent MCP Tool requests")
        print(f"Demonstrated multi-tool concurrent capabilities")

async def main():
    """à¸£à¸±à¸™à¸à¸²à¸£à¸—à¸”à¸ªà¸­à¸š concurrent MCP Tools"""
    
    tester = ConcurrentMCPTester()
    await tester.run_concurrent_tests()

if __name__ == "__main__":
    print("âš¡ Concurrent MCP Tools Test")
    print("Testing multiple MCP Tools simultaneously with performance timing")
    print("=" * 60)
    
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nğŸ‘‹ Concurrent test stopped by user")
    except Exception as e:
        print(f"\nâŒ Error: {e}")
